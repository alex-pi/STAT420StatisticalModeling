---
title: "Week 2 - Homework"
author: "STAT 420, Summer 2021, D. Unger"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

***
```{r}
#options(digits=4)
```

## Exercise 1 (Using `lm`)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Suppose we would like to understand the size of a cat's heart based on the body weight of a cat. Fit a simple linear model in `R` that accomplishes this task. Store the results in a variable called `cat_model`. Output the result of calling `summary()` on `cat_model`.

**Solution:**

```{r}
cats <- MASS::cats
cat_model <- lm(Hwt ~ Bwt, data = cats)
summary(cat_model)
```


**(b)** Output only the estimated regression coefficients. Interpret $\hat{\beta_0}$ and $\beta_1$ in the *context of the problem*. Be aware that only one of those is an estimate.

**Solution:**

```{r}
cat_model$coefficients
```

**$\hat{\beta_0}$** Is the estimated mean heart weight of a weightless cat (when body weight = 0). 
More graphically, is where our fitted line crosses the Y axis.

**$\beta_1$** Is the slope in our model. This is the real unknown value of the slope. In the context of this problem determines the increase of a cat's heart weight (in grams) for each additional kilogram of the cat's body weight.

**(c)** Use your model to predict the heart weight of a cat that weights **3.1** kg. Do you feel confident in this prediction? Briefly explain.

**Solution:**

```{r}
(y_hat_31 <- predict(cat_model, newdata = data.frame(Bwt = 3.1)))
```

*Prediction:* **`r y_hat_31` g.**

The estimated mean weight of a heart is **`r y_hat_31`** grams for a cat with a body weight of **3.1** kgs

Also, **3.1** is a value in the predictors used to fit the model. That gives us a good degree of confidence.

```{r}
3.1 %in% cats$Bwt
```

Let's calculate the residuals for min and max Y (`Hwt`) in the data when x (`Bwt`) is **3.1**

```{r}
(max_y_in_data <- max(cats[cats$Bwt == 3.1, ]$Hwt))
(min_y_in_data <- min(cats[cats$Bwt == 3.1, ]$Hwt))

(res_on_max <- max_y_in_data - y_hat_31)
(res_on_min <- min_y_in_data - y_hat_31)
```

For the max value in Y (**`r max_y_in_data`**) we get a residual of **`r res_on_max`**. This means that the observed heart weight is more than the estimated.

Similarly, the min observed value in Y (**`r min_y_in_data`**) is smaller than the estimated since the residual is negative, **`r res_on_min`**

We can check the Coefficient of Determination for our model.

```{r}
coeff_deter <- summary(cat_model)$r.squared
 
```

**`r coeff_deter * 100`%** of the variance in the heart's weight is explained by the body weight. We can say that our model is somewhat giving good predictions but still **`r (1 - coeff_deter) * 100`%** is noise (other unknown factors).


**(d)** Use your model to predict the heart weight of a cat that weights **1.5** kg. Do you feel confident in this prediction? Briefly explain.

**Solution:**

```{r}
y_hat_15 <- predict(cat_model, newdata = data.frame(Bwt = 1.5))
```

*Prediction:* **`r y_hat_15` g.**

In this case we are extrapolating since **1.5** is out of range of the observed data. Giving that, we should be less confident about this prediction.

```{r}
1.5 < max(cats$Bwt) & 1.5 > min(cats$Bwt)
```

**(e)** Create a scatterplot of the data and add the fitted regression line. Make sure your plot is well labeled and is somewhat visually appealing.

```{r}
plot(Hwt ~ Bwt, data = cats,
     xlab = "Body Weight in kg",
     ylab = "Heart Weight in g.",
     main = "Cats Heart Weight vs Body Weight",
     pch = 20,
     cex = 2,
     col = "grey"
     )
abline(cat_model, lwd = 3, col = "darkorange")
```


**(f)** Report the value of $R^2$ for the model. Do so directly. Do not simply copy and paste the value from the full output in the console after running `summary()` in part **(a)**.

**Solution**

```{r}
summary(cat_model)$r.squared
```


***

## Exercise 2 (Writing Functions)

This exercise is a continuation of Exercise 1.

**(a)** Write a function called `get_sd_est` that calculates an estimate of $\sigma$ in one of two ways depending on input to the function. The function should take three arguments as input:

- `fitted_vals` - A vector of fitted values from a model
- `actual_vals` - A vector of the true values of the response
- `mle` - A logical (`TRUE` / `FALSE`) variable which defaults to `FALSE`

The function should return a single value:

- $s_e$ if `mle` is set to `FALSE`.
- $\hat{\sigma}$ if `mle` is set to `TRUE`.

```{r}
get_sd_est <- function(fitted_vals, actual_vals, mle = FALSE) {
  n <- length(actual_vals)
  e_2 <- sum((actual_vals - fitted_vals) ^ 2)
  ifelse(mle, sqrt(e_2 / n), sqrt(e_2 / (n-2)))
}

```


**(b)** Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `FALSE`. Explain the resulting estimate in the context of the model.

**Solution**

```{r}
y_hat <- cat_model$fitted.values
y <- cats$Hwt
(s_e <- get_sd_est(fitted_vals = y_hat, actual_vals = y))
```

The Residual Standard Error indicates that our estimated mean for the hearts weight is wrong in "average" by `r s_e` grams.

**(c)** Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `TRUE`. Explain the resulting estimate in the context of the model. Note that we are trying to estimate the same parameter as in part **(b)**.

**Solution**

```{r}
(s_d <- get_sd_est(fitted_vals = y_hat, actual_vals = y, mle = TRUE))
```

Using `mle` we get a similar value of `r s_d`. Our mean estimates will be off by `r s_d` grams. For `mle` we use `r nrow(cats)` degrees of freedom and is considered `biased`.

**(d)** To check your work, output `summary(cat_model)$sigma`. It should match at least one of **(b)** or **(c)**.

**Solution**

```{r}
all.equal(summary(cat_model)$sigma, s_e)
all.equal(summary(cat_model)$sigma, s_d)
```

In this case the model's $$\sigma$$ matches the `Least Squares` estimate.

***

## Exercise 3 (Simulating SLR)

Consider the model

\[
Y_i = 5 + -3 x_i + \epsilon_i
\]

with 

\[
\epsilon_i \sim N(\mu = 0, \sigma^2 = 10.24)
\]

where $\beta_0 = 5$ and $\beta_1 = -3$.

This exercise relies heavily on generating random observations. To make this reproducible we will set a seed for the randomization. Alter the following code to make `birthday` store your birthday in the format: `yyyymmdd`. For example, [William Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset), better known as *Student*, was born on June 13, 1876, so he would use:

```{r}
birthday = 19820426
set.seed(birthday)
```

**(a)** Use `R` to simulate `n = 25` observations from the above model. For the remainder of this exercise, use the following "known" values of $x$.

```{r}
x = runif(n = 25, 0, 10)
```

You may use [the `sim_slr ` function provided in the text](http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#simulating-slr). Store the data frame this function returns in a variable of your choice. Note that this function calls $y$ `response` and $x$ `predictor`.

```{r}
sim_slr <- function(x, beta_0 = 10, beta_1 = 5, sigma = 1) {
  n <- length(x)
  epsilon <- rnorm(n = n, mean = 0, sd = sigma)
  y <- beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}
```

**Solution**

```{r}
beta_0 <- 5
beta_1 <- -3
n <- 25
sigma <- sqrt(10.24)
sim_data <- sim_slr(x = x, beta_0 = beta_0, beta_1 = beta_1, sigma = sigma)
```


**(b)** Fit a model to your simulated data. Report the estimated coefficients. Are they close to what you would expect? Briefly explain.

**Solution**

```{r}
sim_model <- lm(response ~ predictor, data = sim_data)
(beta_0_hat <- sim_model$coefficients[1])
(beta_1_hat <- sim_model$coefficients[2])
```

The estimated intercept, $\hat\beta_0$ is not as close as $\beta_0$, they differ by `r beta_0 - beta_0_hat`

On the other hand, the estimated slope $\hat\beta_1$ is close to $\beta_1$ from the model, difference is only `r beta_1 - beta_1_hat`

**(c)** Plot the data you simulated in part **(a)**. Add the regression line from part **(b)** as well as the line for the true model. Hint: Keep all plotting commands in the same chunk.

```{r}
plot_simulation <- function (sim_data, sim_model, beta_0 = 5, beta_1 = -3) {
  plot(response ~ predictor, data = sim_data,
       xlab = "Predictor",
       ylab = "Response",
       main = "SLR Simulation",
       pch = 20,
       cex = 2,
       col = "grey",
       ylim = c(min(response), max(response))
       )
  abline(sim_model, lwd = 3, col = "darkorange")
  abline(beta_0, beta_1, lwd = 3, lty = 2, col = "dodgerblue")
  
  legend("topright", c("Estimate", "Expected"), 
         lty = c(1, 2),
         lwd = 3,
         col = c("darkorange", "dodgerblue"))
}

plot_simulation(sim_data, sim_model)
```


**(d)** Use `R` to repeat the process of simulating `n = 25` observations from the above model $1500$ times. Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. Some hints:

- Consider a `for` loop.
- Create `beta_hat_1` before writing the `for` loop. Make it a vector of length $1500$ where each element is `0`.
- Inside the body of the `for` loop, simulate new $y$ data each time. Use a variable to temporarily store this data together with the known $x$ data as a data frame.
- After simulating the data, use `lm()` to fit a regression. Use a variable to temporarily store this output.
- Use the `coef()` function and `[]` to extract the correct estimated coefficient.
- Use `beta_hat_1[i]` to store in elements of `beta_hat_1`.
- See the notes on [Distribution of a Sample Mean](http://daviddalpiaz.github.io/appliedstats/introduction-to-r.html#distribution-of-a-sample-mean) for some inspiration.

You can do this differently if you like. Use of these hints is not required.

**Solution**

```{r}
loop_simulation <- function (x_vals, num_simulations = 1500, beta_0 = 5, beta_1 = -3, sigma = sqrt(10.24)) {
  beta_hat_1 <- rep(0, num_simulations)
  beta_hat_0 <- rep(0, num_simulations)
  for (i in 1:num_simulations) {
    sim_data <- sim_slr(x = x_vals, beta_0 = beta_0, beta_1 = beta_1, sigma = sigma)
    sim_model <- lm(response ~ predictor, data = sim_data)
    beta_hat_0[i] <- sim_model$coefficients[1]
    beta_hat_1[i] <- sim_model$coefficients[2]
  }
  beta_hat_1
}
```

```{r}
beta_hat_1 <- loop_simulation(x, sigma = sigma)
head(beta_hat_1)
```


**(e)** Report the mean and standard deviation of `beta_hat_1`. Do either of these look familiar?

**Solution**

```{r}
sim_mean <- mean(beta_hat_1)
sim_sd <- sd(beta_hat_1)
```

$\bar\beta_1_hat$ (`r sim_mean`) is close to $\beta_1$ from the model, difference is only `r beta_1 - beta_1_hat`

**(f)** Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.

```{r}
sim_hist <- function(beta_hat_1) {
  hist(beta_hat_1,
       xlab   = "Estimated Beta_1 values",
       main   = "Histogram Beta_1 values from Simulations",
       col  = "deepskyblue3",
       border = "gray80"
       )
}
sim_hist(beta_hat_1)
```

The shape corresponds to a Normal Distribution, its mean seems to be located around `-3.0`. That is, the $\beta_1$ value for our proposed model parameters.

***

## Exercise 4 (Be a Skeptic)

Consider the model

\[
Y_i = 3 + 0 \cdot x_i + \epsilon_i
\]

with

\[
\epsilon_i \sim N(\mu = 0, \sigma^2 = 4)
\]

where $\beta_0 = 3$ and $\beta_1 = 0$.

Before answering the following parts, set a seed value equal to **your** birthday, as was done in the previous exercise.

```{r}
birthday = 19820426
set.seed(birthday)
```

**(a)** Use `R` to repeat the process of simulating `n = 75` observations from the above model $2500$ times. For the remainder of this exercise, use the following "known" values of $x$.

```{r}
x = runif(n = 75, 0, 10)
```

Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. You may use [the `sim_slr ` function provided in the text](http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#simulating-slr). Hint: Yes $\beta_1 = 0$.

**Solution**

```{r}
beta_hat_1 <- loop_simulation(x, num_simulations = 2500, beta_0 = 3, beta_1 = 0, sigma = 2)
```

**(b)** Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.

**Solution**

```{r}
sim_hist(beta_hat_1)
```

In this case since $\beta_1=0$ in our model, most of the estimated values are shown in the histogram very close to `0`. The mean is close to 0 as well: `r mean(beta_hat_1)`

**(c)** Import the data in [`skeptic.csv`](skeptic.csv) and fit a SLR model. The variable names in `skeptic.csv` follow the same convention as those returned by `sim_slr()`. Extract the fitted coefficient for $\beta_1$.

**Solution**

```{r message=FALSE}
library(readr)
skeptic <- read_csv("skeptic.csv")
skeptic_model <- lm(response ~ predictor, skeptic)
skeptic_beta_hat_1 <- skeptic_model$coefficients[2]
```

**(d)** Re-plot the histogram from **(b)**. Now add a vertical red line at the value of $\hat{\beta_1}$ in part **(c)**. To do so, you'll need to use `abline(v = c, col = "red")` where `c` is your value.

**Solution**

```{r}
sim_hist(beta_hat_1)
abline(v = skeptic_beta_hat_1, col = "red")
```

**(e)** Your value of $\hat{\beta_1}$ in **(c)** should be negative. What proportion of the `beta_hat_1` values is smaller than your $\hat{\beta_1}$? Return this proportion, as well as this proportion multiplied by `2`.

**Solution**

```{r}
(proportion <- sum(beta_hat_1 < skeptic_beta_hat_1) / length(beta_hat_1))
proportion * 2
```


**(f)** Based on your histogram and part **(e)**, do you think the [`skeptic.csv`](skeptic.csv) data could have been generated by the model given above? Briefly explain.

```{r}
summary(skeptic_model)

(beta_hat_1_mean <- mean(beta_hat_1))
(beta_hat_1_sd <- sd(beta_hat_1))
beta_hat_1_sd * 2

dnorm(skeptic_beta_hat_1, mean = beta_hat_1_mean, sd = beta_hat_1_sd)
pnorm(skeptic_beta_hat_1, mean = beta_hat_1_mean, sd = beta_hat_1_sd)
# x = mean - (k * sd)
# mean - x = k * sd
# k = (mean - x) / sd
(k_sd_from_mean <- (beta_hat_1_mean - skeptic_beta_hat_1) / beta_hat_1_sd)
```

From the graph we can see $\hat\beta_1$ fitted for the skeptic data set is located `r k_sd_from_mean` Standard Variations from the mean. So the probability for the simulated model to fit a line where $\hat\beta_1$ is close to `r skeptic_beta_hat_1` is very low.

We can calculate the probability of $\hat\beta_1$ to be `r skeptic_beta_hat_1` or less.

```{r}
beta_hat_1_mean <- mean(beta_hat_1)
beta_hat_1_sd <- sd(beta_hat_1)
pnorm(skeptic_beta_hat_1, mean = beta_hat_1_mean, sd = beta_hat_1_sd)
```

We can extend the idea and say that using the simulation model to fit a line that explains the skeptic data set has very low probability, then the model also has very low probability to produce such data set.

***

## Exercise 5 (Comparing Models)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will perform some data cleaning before proceeding.

```{r message=FALSE}
?Ozone
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

We have:

- Loaded the data from the package
- Subset the data to relevant variables
    - This is not really necessary (or perhaps a good idea) but it makes the next step easier
- Given variables useful names
- Removed any observation with missing values
    - This should be given much more thought in practice

For this exercise we will define the "Root Mean Square Error" of a model as

\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}.
\]

**(a)** Fit three SLR models, each with "ozone" as the response. For the predictor, use "wind speed," "humidity percentage," and "temperature" respectively. For each, calculate $\text{RMSE}$ and $R^2$. Arrange the results in a markdown table, with a row for each model. Suggestion: Create a data frame that stores the results, then investigate the `kable()` function from the `knitr` package.

```{r}
library(knitr)

model_wind <- lm(ozone ~ wind, data = Ozone)
model_humi <- lm(ozone ~ humidity, data = Ozone)
model_temp <- lm(ozone ~ temp, data = Ozone)

calculate_rmse <- function(model) {
  sqrt(sum(model$residuals ^ 2) / (nrow(model$model)))
}

results <- data.frame(
  row.names = c("Wind","Humidity","Temp"),
  "r_squared" = c(summary(model_wind)$r.squared,
                  summary(model_humi)$r.squared,
                  summary(model_temp)$r.squared),
  "RMSE" = c(calculate_rmse(model_wind),
             calculate_rmse(model_humi),
             calculate_rmse(model_temp))
)

kable(results, 
      col.names = c("R^2", "RMSE"),
      caption = "Comparing Ozone model with 3 different predictors")
```


**(b)** Based on the results, which of the three predictors used is most helpful for predicting ozone readings? Briefly explain.

The temperature explains `r results[3, ]$r_squared * 100`% of the variability in the Ozone measures. It also has the lowest "Root Mean Square Error". Given that, we can conclude that the Temperature is the most useful.

***