---
title: 'Week 6 - Midterm Assignment: A Simulation Project'
author: "STAT 420, Summer 2021,Alejandro Pimentel (ap41)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
    code_folding: show
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

```{css, echo=FALSE}
p, li, td {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
```

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(knitr)
library(kableExtra)
library(dplyr)

format_numerics <- function(data, digits = 2, notation_threshold = 0.00001) {
  # make sure is a data.frame, then format
  if(!is.data.frame(data)){
    data <- as.data.frame(data)
  }  
  data %>% 
    mutate_if(
      is.numeric, 
      function(x) {
        if_else(
          abs(x) < notation_threshold, 
          formatC(x, digits = digits, format = "e"), 
          formatC(x, digits = digits, format = "f", drop0trailing = FALSE)
        )
      }
    )
}

gen_kable <- function(table_data, add_row_names = TRUE, caption = "", col_names = c(), row_names = c()) {
  f_data <- format_numerics(table_data) 
  if(length(col_names) != 0){
    colnames(f_data) <- col_names
  }
  if(length(row_names) != 0){
    rownames(f_data) <- row_names
  }  
  f_data %>%
  kable(., format = "html", row.names = add_row_names,
        caption = caption, escape = FALSE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"),
                  full_width = F,
                  font_size = 14,
                  position = "center")
}

fr <- function(nums, digits = 5, notation_threshold = 0.000001) {
  if_else(
    abs(nums) < notation_threshold, 
    formatC(nums, digits = digits, format = "e"), 
    format(round(nums, digits), nsmall = digits)
  )
}

```

# Simulation Study 2: Using RMSE for Selection?

## Introduction

In this simulation study we will investigate how well RMSE can be used to select the “best” model. Since splitting the data is random, we don’t expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

We will use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These will be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time we simulate the data, we randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, we fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, we calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

We will repeat this process with $1000$ simulations for each of the $3$ values of $\sigma$. 

## Methods

Read the `csv` file with our samples and take a look to some of them.

```{r message=FALSE, warning=FALSE}
library(readr)
study2_data <- read_csv("./study_2.csv")

head(study2_data)

```

```{r}
# Set a seed to have a stable generation of random numbers
birthday = 19820426
set.seed(birthday)
```

We start by defining a few functions to help with the overall simulation. Namely, to help us with the tasks:

- Calculate the `RMSE`.
- Generate simulation data.
- Get the `RMSE` values and minimum values for each of the `9` models.

```{r}
rmse <- function(expected, predicted) {
  n <- length(expected)
  sqrt(sum((expected - predicted)^2) / n)
}

# Generate the simulation data, concretely the y values given an epsilon
# and the known model with 6 predictors.
simulate_data <- function(X, betas, sigma) {
  n <- nrow(X)
  epsilon <- rnorm(n = n, mean = 0, sd = sigma)
  X_full <- as.matrix(cbind(1, X[, 2:7]))
  X$y <- (X_full %*% betas) + epsilon
  as.data.frame(X)
}

# For each of the 9 models, this function calculates the RMSE 
# for train an test data as well as the index of the minimum RMSE on each case.
collect_rmse <- function(trn_data, tst_data) {
  # row 1 contains rmse values for train data on the 9 models
  # row 2 contains rmse values for test data on the 9 models
  rmse_matrix <- matrix(-1, 2, 9)

  for(im in 1:9) {
    # We take a bigger slice of the predictors until we reach 9
    trn_mod <- trn_data[, 1:(im+1)]
    tst_mod <- tst_data[, 1:(im+1)]
    # We train the model with the train chunk ONLY
    sim_model <- lm(y ~ ., data = trn_mod)
    rmse_matrix[1, im] <- rmse(trn_mod$y, sim_model$fitted.values)
    # We predict for the test chunk
    tst_predictions <- predict(sim_model, newdata = tst_mod)
    rmse_matrix[2, im] <- rmse(tst_mod$y, tst_predictions)
  }
  list(
    rmse_matrix = rmse_matrix,
    # The index indicates the model number with the smallest RMSE
    trn_min_idx = which.min(rmse_matrix[1, ]),
    tst_min_idx = which.min(rmse_matrix[2, ])
    )
}

```

Next, we pre-allocate variables we will use during the simulation.

```{r}
betas <- matrix(c(0, 3, -4, 1.6, -1.1, 0.7, 0.5))
sigma1 <- 1
sigma2 <- 2
sigma4 <- 4
num_sims <- 1000

rmse_s1_acc <- matrix(0, 2, 9)
rmse_s2_acc <- matrix(0, 2, 9)
rmse_s4_acc <- matrix(0, 2, 9)

# For each simulation we save the index of the model with the smallest RMSE
# We do that for both train and test data and each sigma
which_mins_s1 <- matrix(0, 2, num_sims)
which_mins_s2 <- matrix(0, 2, num_sims)
which_mins_s4 <- matrix(0, 2, num_sims)

```

The loop below runs the simulation and collects all the data. As described in the introduction: 

Each time we simulate the data, we randomly split the data into `train` and `test` sets of equal sizes (250 observations for training, 250 observations for testing).

Then for each model, we calculate Train and Test RMSE.

```{r}
# The main loop with num_sims simulations

for(i in 1:num_sims) {

  # We simulate the data for the true model (with 7 betas).
  # For each sigma we get a different data set.
  data_sigma1 <- simulate_data(study2_data, betas, sigma1)
  data_sigma2 <- simulate_data(study2_data, betas, sigma2)
  data_sigma4 <- simulate_data(study2_data, betas, sigma4)
  
  # Get 250 random indexes for observations
  idxs <- 1:nrow(study2_data)
  trn_idx = sample(idxs, 250)
  
  # Training splits for each sigma
  data_sigma1_trn <- data_sigma1[trn_idx, ]
  data_sigma2_trn <- data_sigma2[trn_idx, ]
  data_sigma4_trn <- data_sigma4[trn_idx, ]
  
  # Testing splits for each sigma
  data_sigma1_tst <- data_sigma1[!(idxs %in% trn_idx), ]
  data_sigma2_tst <- data_sigma2[!(idxs %in% trn_idx), ]
  data_sigma4_tst <- data_sigma4[!(idxs %in% trn_idx), ]
  
  # We get a list with test and train RMSE and the index with 
  # then min RMSE for both cases
  rmse_sigma1 <- collect_rmse(data_sigma1_trn, data_sigma1_tst)
  rmse_sigma2 <- collect_rmse(data_sigma2_trn, data_sigma2_tst)
  rmse_sigma4 <- collect_rmse(data_sigma4_trn, data_sigma4_tst)
  
  # Accumulate the rmse in a 2 by 9 matrix, 
  # we'll use it later to get the portion of correct model selection
  rmse_s1_acc <- rmse_s1_acc + rmse_sigma1$rmse_matrix
  rmse_s2_acc <- rmse_s2_acc + rmse_sigma2$rmse_matrix
  rmse_s4_acc <- rmse_s4_acc + rmse_sigma4$rmse_matrix
  
  # Save the index of the minimum RMSE for test/train and each sigma
  which_mins_s1[1, i] <- rmse_sigma1$trn_min_idx
  which_mins_s1[2, i] <- rmse_sigma1$tst_min_idx
  
  which_mins_s2[1, i] <- rmse_sigma2$trn_min_idx
  which_mins_s2[2, i] <- rmse_sigma2$tst_min_idx
  
  which_mins_s4[1, i] <- rmse_sigma4$trn_min_idx
  which_mins_s4[2, i] <- rmse_sigma4$tst_min_idx  

}


```


## Results

In this section we use a few graphs to summarize the results obtained in the simulations. The functions below will help us to:

- Plot the `Average RMSE` for `test` and `train` splits for each proposed `sigma`.
- Create a bar plot to show how many times each model got the lowest `RMSE`.

```{r}
line_plot <- function(y1, y2, sigma, title, show_ylab = FALSE) {
  ylab <- ifelse(show_ylab, paste("Average RMSE (", num_sims, "simulations )"), "")
  plot(1:9, y1,
       xlab = "Model size (number of predictors)", 
       ylab = ylab,
       #main = paste(title," ( sigma =", sigma,")"),     
       main = bquote(.(title) ~ " (" ~ sigma ~ "=" ~ .(sigma) ~ ")"),
       col = "darkorange",
       lwd=2.5,
       lty=2,
       type='o')
  
  lines(1:9,y2,
        lwd=2.5,
        lty=2,        
        col='forestgreen',
        type='o')
  
  legend("topright", c("Train Data", "Test Data"), 
       lty = c(2, 2),
       lwd = 3,
       col = c("darkorange", "forestgreen"))  
}

bar_plot <- function(data, sigma) {
  td <- table(data)
  barplot(td,
    main=paste("Model Selection based on RMSE (sigma = ", sigma," )"),
    xlab="Number of Predictors",
    ylab="Model Selection Count",
    border="gray",
    col='deepskyblue3',
    density=500,
    ylim = c(0, 600)
  )
}

```



We are interested in observing how the `RMSE` changes depending on the `number of predictors` or `model size`. We plot that for each of our `sigma` values.

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
t <- "Avg. RMSE vs Model Size"
line_plot(rmse_s1_acc[1, ] / num_sims, rmse_s1_acc[2, ] / num_sims,
          sigma = 1, title = t, show_ylab = TRUE)
line_plot(rmse_s2_acc[1, ] / num_sims, rmse_s2_acc[2, ] / num_sims,
          sigma = 2, title = t)
line_plot(rmse_s4_acc[1, ] / num_sims, rmse_s4_acc[2, ] / num_sims,
          sigma = 4, title = t)
```

**Graph 2.1 - Comparing Average RMSE for 3 sigma values**

Another important aspect to check is, which models are getting the `lowest` RMSE. The bar plots below show that for each `sigma` and the `test data`.

```{r fig.height = 6, fig.width = 10, fig.align = "center"}

par(mfrow=c(1, 3), bg="ghostwhite")
bar_plot(which_mins_s1[2, ], sigma1)
bar_plot(which_mins_s2[2, ], sigma2)
bar_plot(which_mins_s4[2, ], sigma4)
```

**Graph 2.2 - Fitted models with lowest RMSE**

The table below presents the exact proportion of `simulations` in which `model 6` (our true known model) is getting the lowest `RMSE`.

```{r}
rmse_sel <- c(mean(which_mins_s1[2, ] == 6),
              mean(which_mins_s2[2, ] == 6),
              mean(which_mins_s4[2, ] == 6)
              )

gen_kable(rmse_sel, col_names = c("Proportion"), 
          caption = "Proportion of Simulations selecting model 6.",
          row_names = c("sigma = 1", "sigma = 2", "sigma = 4"))

```

**Table 2.1 - Correct model selection for each** $\sigma$

Lastly, we notice below that in the case of the `train data`, the smallest `RMSE` always corresponds to the `largest model`, in this case the `model` with 9 `predictors`.

```{r}
# The values below correspond to the total number of simulations.
c(sum(which_mins_s1[1, ] == 9),
  sum(which_mins_s2[1, ] == 9),
  sum(which_mins_s4[1, ] == 9))

```


## Discussion

**The method of lowest RMSE does not always select the true model**

From `Table 2.1` we can see that the proportion of times we selected `model 6` is not `1`, meaning that in the experiment we are not always selecting the `true model`. For instance, for $\sigma = 1$ we select it only for `r (mean(which_mins_s1[2, ] == 6) * 100)`% of the `r num_sims` simulations.

Furthermore, from `Graph 2.2` we can see other models getting the lowest `RMSE` for different values of $\sigma$. Notably, the models close to the `true model` get sometimes the smallest `RMSE`.

`Graph 2.1` shows an important detail, the `Avg RMSE` seems to reach a minimum for `model 6` (the `true model`), after that, it tends to go slightly up. So we can state that in average the lowest `RMSE` will correspond to the `true model`.

**The level of noise affects the RMSE values**

Recall that the `RMSE` depends on the `Sum of Squared Error` which represents the unexplained variation of $y$. By introducing more noise (i.e. a larger $\sigma$) we are less confident of our estimates for $\hat{y}_i$, this increases the distance between our estimates and the true value of $y$. So the term $y_i - \hat{y}_i$ becomes bigger causing a bigger `RMSE` value.

$$
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
$$

From `Graph 2.1` we can see that the `Average RMSE` increases as we increase the $\sigma$ value. This is true for both `Train` and `Test` data.

Furthermore, more noise can also affect the selection of the `true model`. By looking at `Graph 2.2` we can see that as $\sigma$ increases, other models get selected more frequently.

Lastly, from `Graph 2.1` we can see that the Average `RMSE` for the `train data` only goes lower the more predictors are in the model, this a similar but opposite effect as the one observed for $R^2$, where it increases as more predictors are added to the model.

