sim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3) {
x = rnorm(n = sample_size)
eta = beta_0 + beta_1 * x
p = 1 / (1 + exp(-eta))
y = rbinom(n = sample_size, size = 1, prob = p)
data.frame(y, x)
}
set.seed(1)
example_data = sim_logistic_data()
head(example_data)
# ordinary linear regression
fit_lm  = lm(y ~ x, data = example_data)
# logistic regression
fit_glm = glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
predict(fit_glm, newdata = data.frame(x = 1.2), type = "link")
predict(fit_glm, newdata = data.frame(x = 1.2), type = "response")
plot(y ~ x, data = example_data,
pch = 20, ylab = "Estimated Probability",
main = "Ordinary vs Logistic Regression")
grid()
abline(fit_lm, col = "darkorange")
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
legend("topleft", c("Ordinary", "Logistic", "Data"), lty = c(1, 2, 0),
pch = c(NA, NA, 20), lwd = 2, col = c("darkorange", "dodgerblue", "black"))
set.seed(42)
intercept <- 0
slope <- 1
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Estimated Probability",
main = "Ordinary vs Logistic Regression")
grid()
abline(fit_lm, col = "darkorange")
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
legend("topleft", c("Ordinary", "Logistic", "Data"), lty = c(1, 2, 0),
pch = c(NA, NA, 20), lwd = 2, col = c("darkorange", "dodgerblue", "black"))
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
abline(fit_glm, col = "darkorange")
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
legend("topleft", c("Ordinary", "Logistic", "Data"), lty = c(1, 2, 0),
pch = c(NA, NA, 20), lwd = 2, col = c("darkorange", "dodgerblue", "black"))
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
abline(fit_glm, col = "darkorange")
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x), type = "response"),
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
abline(fit_glm, col = "darkorange")
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "dodgerblue", lty = 1)
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
intercept <- -5
slope <- 1
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
intercept <- -2
slope <- 1
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
intercept <- 2
slope <- 1
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
# The intercept seems to have an effect in the proportion of 1s and 0s in the response
# Positive intercept (2) will generate more data points with response 1.
intercept <- 0
slope <- 1
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
# The intercept seems to have an effect in the proportion of 1s and 0s in the response
# Positive intercept (2) will generate more data points with response 1.
intercept <- 0
slope <- 2
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
# The intercept seems to have an effect in the proportion of 1s and 0s in the response
# Positive intercept (2) will generate more data points with response 1.
intercept <- 0
# The larger in magnitud the slope is, the sharper the change of probaility from 0 to 1 is.
# Same for negative, in that case it would be decrease of probability is sharper the mode negative the slope is.
slope <- -2
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
# The intercept seems to have an effect in the proportion of 1s and 0s in the response
# Positive intercept (2) will generate more data points with response 1.
intercept <- 0
# The larger in magnitud the slope is, the sharper the change of probaility from 0 to 1 is.
# Same for negative, in that case it would be decrease of probability is sharper the mode negative the slope is.
slope <- -4
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
# The intercept seems to have an effect in the proportion of 1s and 0s in the response
# Positive intercept (2) will generate more data points with response 1.
intercept <- 0
# The larger in magnitud the slope is, the sharper the change of probaility from 0 to 1 is.
# Same for negative, in that case it would be decrease of probability is sharper the mode negative the slope is.
slope <- -20
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
# The intercept seems to have an effect in the proportion of 1s and 0s in the response
# Positive intercept (2) will generate more data points with response 1.
intercept <- 0
# The larger in magnitud the slope is, the sharper the change of probaility from 0 to 1 is.
# Same for negative, in that case it would be decrease of probability is sharper the mode negative the slope is.
slope <- -2
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
plot(y ~ x, data = example_data,
pch = 20, ylab = "Probability",
main = "True: Orange solid, Estimated: Blue, Dashed")
grid()
curve(predict(fit_glm, data.frame(x), type = "response"),
add = TRUE, col = "dodgerblue", lty = 2)
curve(boot::inv.logit(intercept + slope * x),
add = TRUE, col = "darkorange", lty = 1)
set.seed(42)
# The intercept seems to have an effect in the proportion of 1s and 0s in the response
# Positive intercept (2) will generate more data points with response 1.
intercept <- 10
# The larger in magnitud the slope is, the sharper the change of probaility from 0 to 1 is.
# Same for negative, in that case it would be decrease of probability is sharper the mode negative the slope is.
slope <- -10
example_data <- sim_logistic_data(sample_size = 50, beta_0 = intercept, beta_1 = slope)
fit_glm <- glm(y ~ x, data = example_data, family = binomial)
fit_glm$coefficients
x1 <- 1
x2 <- 0
(Y = 2 + (-1)*x1 + (-1)*x2)
boot::inv.logit(Y)
1/(1 + exp(-Y))
?pibinom
?pbinom
pbinom(Y)
pbinom(Y, size = 1)
pbinom(Y, size = 1, prob = 0.5)
Y
?pchi
?pchisq
pchisq(2.546257, df = 5)
pchisq(2.546257, df = 5, log.p = TRUE)
pchisq(2.546257, df = 5)
pchisq(2.546257, df = 5, npc = 10)
pchisq(2.546257, df = 5, ncp = 10)
pchisq(2.546257, df = 5, ncp = 5)
pchisq(2.546257, df = 5)
pchisq(2.546257, df = 5, lower.tail = FALSE)
?cv.glm
2*.5
3*.25
(Y <- -1 -1 + 1 + 0.75)
(Y <- -3 -1 + 1 + 0.75)
1 / (1 + exp(-Y))
1 - (1 / (1 + exp(-Y)))
mod2 <- glm(am ~ mpg + hp + qsec, data = mtcars, family = binomial)
mod2$coefficients
mod2$coefficients[4]
mod2$coefficients["mpg"]
predict(mod2, newdata = data.frame(
mpg = 19,
hp = 150,
qsec = 19
), type = "link")
predict(mod2, newdata = data.frame(
mpg = 19,
hp = 150,
qsec = 19
), type = "response")
levels(mtcars$am)
mtcars$am
?mtcars
predict(mod2, newdata = data.frame(
mpg = 19,
hp = 150,
qsec = 19
), type = "link")
predict(mod2, newdata = data.frame(
mpg = 22,
hp = 123,
qsec = 18
), type = "link")
predict(mod2, newdata = data.frame(
mpg = 22,
hp = 123,
qsec = 18
), type = "response")
anova(y ~ 1, mod2, test = "LTR")
null_mod <- lm(y ~ 1, data = mtcars, family = binomial)
anova(null_mod, mod2, test = "LTR")
null_mod <- lm(am ~ 1, data = mtcars, family = binomial)
null_mod <- glm(am ~ 1, data = mtcars, family = binomial)
anova(null_mod, mod2, test = "LTR")
anova(null_mod, mod2, test = "LRT")
anov <- anova(null_mod, mod2, test = "LRT")
summary(anov)
names(summary(anov))
summary(anov)
names(anov)
anov$Deviance
anov$Deviance[2]
(devian <- -2 * as.numeric(logLik(null_mod) - logLik(mod2)))
summary(mod2)$coef
summary(mod2)$coef[4, "Pr(>|z|)"]
summary(mod2)$coef[2, "Pr(>|z|)"]
summary(mod2)
43.2297 - 7.4802
summary(mod2)$coef
summary(mod2)$coef[2, "Pr(>|z|)"]
?MASS::Pima.tr
ptr <- MASS::Pima.tr
head(ptr)
mod8 <- glm(type ~ (glu + ped)^2, data = ptr, family = binomial)
summary(mod8)
mod8 <- glm(type ~ (glu + ped)^2 + I(glu^2) + + I(ped^2), data = ptr, family = binomial)
summary(mod8)
mod8$coef
mod8$coef[5]
trn <- MASS::Pima.tr
head(trn)
mod8 <- glm(type ~ (glu + ped)^2 + I(glu^2) + + I(ped^2),
data = trn, family = binomial)
summary(mod8)
mod8$coef[5]
tes <- MASS::Pima.te
predict(mod8, newdata = tes, type = "response")
mean(predict(mod8, newdata = tes, type = "response") > 0.80)
mean(predict(mod8, newdata = tes, type = "response") > 0.50)
mean(predict(mod8, newdata = tes, type = "response") > 0.80)
mod10 <- glm(type ~ ., data = trn, family = binomial)
step(mod10, direction = "backward")
step(mod10, direction = "backward", trace = 0)
mod10_small <- step(mod10, direction = "backward", trace = 0)
anov <- anova(mod10_small, mod10, test = "LRT")
(anov <- anova(mod10_small, mod10, test = "LRT"))
mod11 <- glm(type ~ .^2, data = trn, family = binomial)
mod11_small <- step(mod10, direction = "backward", trace = 0)
mod11_small
mod11_small <- step(mod10, direction = "backward")
mod11 <- glm(type ~ .^2, data = trn, family = binomial)
mod11_small <- step(mod11, direction = "backward")
mod11_small
names(mod11_small)
mod11_small$deviance
mod11_small
mod11_small$deviance
# fit the models here
library(boot)
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(tes, mod8, K = 5)$delta[1]
cv.glm(tes, mod8, K = 5)
mod8 <- glm(type ~ (glu + ped)^2 + I(glu^2) + + I(ped^2),
data = trn, family = binomial)
# get cross-validated results for the polynomial model here
cv.glm(MASS::Pima.te, mod8, K = 5)$delta[1]
mod8 <- glm(type ~ (glu + ped)^2 + I(glu^2) + + I(ped^2),
data = trn, family = binomial)
summary(mod8)
# get cross-validated results for the polynomial model here
cv.glm(MASS::Pima.te, mod8, K = 5)$delta[1]
# get cross-validated results for the polynomial model here
cv.glm(MASS::Pima.tr, mod8, K = 5)$delta[1]
# get cross-validated results for the polynomial model here
cv.glm(trn, mod8, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(trn, mod8, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(trn, mod8, K = 5)$delta[1]
mod8_small <- step(mod8, direction = "backward",
k = log(nrow(trn)), trace = 0)
library(boot)
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(trn, mod8_small, K = 5)$delta[1]
library(boot)
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(trn, mod8, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the additive model here
cv.glm(trn, mod10, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from additive model here
cv.glm(trn, mod10_small, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the interaction model here
cv.glm(trn, mod11, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from interaction model here
cv.glm(trn, mod11_small, K = 5)$delta[1]
library(boot)
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(trn, mod8, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the additive model here
cv.glm(trn, mod10, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from additive model here
cv.glm(trn, mod10_small, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the interaction model here
cv.glm(trn, mod11, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from interaction model here
cv.glm(trn, mod11_small, K = 5)$delta[1]
head(trn)
mod10_small2 <- step(mod10, direction = "backward",
k = log(nrow(quiz_data)), trace = 0)
mod10_small2 <- step(mod10, direction = "backward",
k = log(nrow(trn)), trace = 0)
cv.glm(tes, mod10_small2, K = 5)$delta[1]
ifelse(predict(mod10_small2, spam_tst, type = "response") > 0.5,
"Yes",
"No")
ifelse(predict(mod10_small2, tes, type = "response") > 0.5,
"Yes",
"No")
ifelse(predict(mod10_small2, tes, type = "response") > 0.5, 1, 0)
mean(ifelse(predict(mod10_small2, tes, type = "response") > 0.5, 1, 0))
pred <- ifelse(predict(mod10_small2, tes, type = "response") > 0.5,
"Yes", "No"))
pred <- ifelse(predict(mod10_small2, tes, type = "response") > 0.5,
"Yes", "No")
pred
sum(pred != tes$type)
mean(pred != tes$type)
(confusion <- table(predicted = pred, actual = tes$type))
confusion[2, 2]
confusion[2, 2] / confusion[, 2]
confusion[2, 2] / confusion[, 2]
confusion[2, 2] / sum(confusion[, 2])
pred3 <- ifelse(predict(mod10, tes, type = "response") > 0.3,
"Yes", "No")
(confusion3 <- table(predicted = pred3, actual = tes$type))
pred3 <- ifelse(predict(mod10, tes, type = "response") > 0.3,
"Yes", "No")
(confusion3 <- table(predicted = pred3, actual = tes$type))
confusion3[2, 2] / sum(confusion3[, 2])
summary(mod2)$coef
summary(mod2)$coef[2, "Pr(>|z|)"]
summary(mod2)$coef
summary(mod2)$coef[3, "Pr(>|z|)"]
summary(mod2)$coef
summary(mod2)$coef[3, "Pr(>|z|)"]
pred <- ifelse(predict(mod10, tes, type = "response") > 0.5,
"Yes", "No")
mean(pred != tes$type)
(confusion <- table(predicted = pred, actual = tes$type))
confusion[2, 2] / sum(confusion[, 2])
mod2 <- glm(y ~ ., data = quiz_data, family = "binomial")
mod2_small <- step(mod2, direction = "backward",
k = log(nrow(quiz_data)), trace = 0)
(anov <- anova(mod2_small, mod2, test = "LRT"))
# Question 1
x1 <- 1
x2 <- 0
(Y = 2 + (-1)*x1 + (-1)*x2)
boot::inv.logit(Y)
1/(1 + exp(-Y))
pbinom(Y, size = 1, prob = 0.5)
# Question 2
colnames(quiz_data)
mod2 <- glm(y ~ ., data = quiz_data, family = "binomial")
mod2$coef
mod2$coef[3]
# Question 3
mod2 <- glm(y ~ ., data = quiz_data, family = "binomial")
summary(mod2)$coef
summary(mod2)$coef[4, "Pr(>|z|)"]
# Question 4
mod2 <- glm(y ~ ., data = quiz_data, family = "binomial")
mod2_small <- step(mod2, direction = "backward",
k = log(nrow(quiz_data)), trace = 0)
(anov <- anova(mod2_small, mod2, test = "LRT"))
anov[2, "Pr(>Chi)"]
devian <- -2 * as.numeric(logLik(mod2_small) - logLik(mod2))
pchisq(devian, df = 5, lower.tail = FALSE)
# Question 5
#quiz_data
mod2 <- glm(y ~ ., data = quiz_data, family = binomial)
#why do I need to use BIC in here?
mod2_small <- step(mod2, direction = "backward",
k = log(nrow(quiz_data)), trace = 0)
library(boot)
set.seed(1)
cv.glm(quiz_data, mod2_small, K = 5)$delta[1]
