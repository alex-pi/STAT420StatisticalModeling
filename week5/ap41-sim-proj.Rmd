---
title: 'Week 6 - Midterm Assignment: A Simulation Project'
author: "STAT 420, Summer 2021, Alejandro Pimentel (ap41)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
    code_folding: show
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

```{css, echo=FALSE}
p, li, td {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
```

*Utility functions defined here, click `Code` to see*

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(knitr)
library(kableExtra)
library(dplyr)

format_numerics <- function(data, digits = 2, notation_threshold = 0.00001) {
  # make sure is a data.frame, then format
  if(!is.data.frame(data)){
    data <- as.data.frame(data)
  }  
  data %>% 
    mutate_if(
      is.numeric, 
      function(x) {
        if_else(
          abs(x) < notation_threshold, 
          formatC(x, digits = digits, format = "e"), 
          formatC(x, digits = digits, format = "f", drop0trailing = FALSE)
        )
      }
    )
}

gen_kable <- function(table_data, add_row_names = TRUE, caption = "", col_names = c(), row_names = c()) {
  f_data <- format_numerics(table_data) 
  if(length(col_names) != 0){
    colnames(f_data) <- col_names
  }
  if(length(row_names) != 0){
    rownames(f_data) <- row_names
  }  
  f_data %>%
  kable(., format = "html", row.names = add_row_names,
        caption = caption, escape = FALSE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"),
                  full_width = F,
                  font_size = 14,
                  position = "center")
}

fr <- function(nums, digits = 5, notation_threshold = 0.000001) {
  if_else(
    abs(nums) < notation_threshold, 
    formatC(nums, digits = digits, format = "e"), 
    format(round(nums, digits), nsmall = digits)
  )
}

```

# Simulation Study 1: Significance of Regression

## Introduction

In this simulation study we will investigate the significance of regression test. We will simulate from two different models:

1. The **"significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$.


2. The **"non-significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

For both, we will consider a sample size of $25$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 25$
- $\sigma \in (1, 5, 10)$

Using simulation we will obtain an empirical distribution for each of the following values, for each of the three values of $\sigma$, for both models.

- The **$F$ statistic** for the significance of regression test.
- The **p-value** for the significance of regression test
- **$R^2$**

For each model and $\sigma$ combination, we use $2000$ simulations. For each simulation, we fit a regression model of the same form used to perform the simulation.

The data found in [`study_1.csv`](study_1.csv) has the values of the predictors. These is kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

## Methods

Read the `csv` file with our samples and take a look to some of them.

```{r message=FALSE, warning=FALSE}
library(readr)
study1_data <- read_csv("./study_1.csv")

head(study1_data)

```

```{r}
# Set a seed to have a stable generation of random numbers
birthday = 19820426
set.seed(birthday)
```

First, let's define some constant values for our simulation.

```{r}
# Only take the predictor columns x_i
X <- study1_data[, 2:4]
# The beta values for the significant model
sig_betas <- c(3, 1, 1, 1)
# The beta values for the non-significant model
nosig_betas <- c(3, 0, 0, 0)
# The 3 sigma values to use on each model
sigma_1 <- 1
sigma_5 <- 5
sigma_10 <- 10
num_sims <- 2000

# The degrees of freedom provided we know we have 4 parameters in both models
df1 <- length(sig_betas) - 1
df2 <- nrow(study1_data) - length(sig_betas)
```

For the simulations we need to define a few functions.

First, a function to generate the responses $y$. For which we need:

- A matrix with the predictors values.
- A matrix of `p` by 1 where `p` is the number of parameters $\beta$ in the model.
- A `sigma` to generate noise ($\epsilon$) for the response $y$.

The output is a `data frame` with all predictors and $y$ values added.

```{r}
generate_mlr_data <- function(X, betas, sigma = 1) {
  n <- nrow(X)
  epsilon <- rnorm(n = n, mean = 0, sd = sigma)
  X_full <- as.matrix(cbind(1, X))
  X$y <- (X_full %*% betas) + epsilon
  as.data.frame(X)
}

```

Next, we need a function that runs a number of simulations with fixed $\beta_i$ values and $\sigma$. For each `fitted model` the function extracts:

- The **$F$ statistic** for the significance of regression test.
- The **p-value** for the significance of regression test.
- **$R^2$**

```{r}
loop_simulation <- function (X, betas, num_simulations = 2000, sigma = 1) {
  # pre-allocate vector to store values
  fstats <- rep(0, num_simulations)
  pvals <- rep(0, num_simulations)
  r2s <- rep(0, num_simulations)
  for (i in 1:num_simulations) {
    sim_data <- generate_mlr_data(X, matrix(betas), sigma)
    sim_model <- lm(y ~ ., data = sim_data)
    ft <- unname(summary(sim_model)$fstatistic)
    # Extract/Calculate F-stat,p-value and r^2
    fstats[i] <- ft[1]
    pvals[i] <- 1 - pf(abs(ft[1]), df1 = ft[2], df2 = ft[3])
    r2s[i] <- summary(sim_model)$r.squared
  }
  list(
    fstats = fstats,
    pvals = pvals,
    r2s = r2s
    )
}

```

We are set to run the 6 simulations, the code below does that for each model and $\sigma$.

```{r}

# Simulations for the significant model 
signif_1 <- loop_simulation(X, sig_betas, num_simulations = num_sims, sigma_1)
signif_5 <- loop_simulation(X, sig_betas, num_simulations = num_sims, sigma_5)
signif_10 <- loop_simulation(X, sig_betas, num_simulations = num_sims, sigma_10)

# Simulations for the non-significant model 
nosignif_1 <- loop_simulation(X, nosig_betas, num_simulations = num_sims, sigma_1)
nosignif_5 <- loop_simulation(X, nosig_betas, num_simulations = num_sims, sigma_5)
nosignif_10 <- loop_simulation(X, nosig_betas, num_simulations = num_sims, sigma_10)

```

## Results

Here we present a collection of graph to analyze the simulation results. To keep things DRY I define a function that shows:

- A `histogram` of the empirical values obtained in the simulation.
- The `true` curve of the distribution (when the distribution is known).

```{r}
graph_helper <- function(simuldata, sigma, dist_curve = "", 
                         color, title, auto_xlim = TRUE) {
  # TODO change to ggplot as it seems like conditionally
  # setting properties is easier and more idiomatic.
  if(auto_xlim) {
    hist(simuldata, probability = TRUE, breaks = 15,
       xlab = title, 
       main = bquote(.(title) ~ " (" ~ sigma  ~ "=" ~ .(sigma) ~")"), 
       col = color,
       border = "aliceblue")
  } else {
    hist(simuldata, probability = TRUE, breaks = 15,
       xlab = title, 
       xlim = c(-1, max(simuldata)),
       main = bquote(.(title) ~ " (" ~ sigma  ~ "=" ~ .(sigma) ~")"), 
       col = color,
       border = "aliceblue")    
  }
  # The curve for F distribution
  if(dist_curve == "f") {
    curve(df(x, df1 = df1, df2 = df2),
        col = "darkorange", 
        add = TRUE, lwd = 3
        )
  }
  
  # The curve of a uniform distribution
  if(dist_curve == "unif") {
    curve(dunif(x),
      col = "darkorange", 
      add = TRUE, lwd = 3
      )
  }
  
  # Labels needed when showing more than one curve/hist
  if(dist_curve != "") {
    legend("topright", c("Empirical", "Real"), 
         lty = c(1, 2),
         lwd = 3,
         col = c(color, "darkorange"))
  }
}

```


**We use color blue to show the empirical distributions of the `significant model`. Gray corresponds to the `non-significant model`.**

### Set of graphs for F-statistic distributions

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
title <- "F Statistic Distribution"
graph_helper(signif_1$fstats, sigma_1, "f", color = "deepskyblue3", title, 
             auto_xlim = FALSE)
graph_helper(signif_5$fstats, sigma_5, "f", color = "deepskyblue3", title)
graph_helper(signif_10$fstats, sigma_10, "f", color = "deepskyblue3", title)
```

**Graph 1.1 - F Statistics for the `significant model` on each $\sigma$**

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
title <- "F Statistic Distribution"
graph_helper(nosignif_1$fstats, sigma_1, "f", color = "darkgray", title)
graph_helper(nosignif_5$fstats, sigma_5, "f", color = "darkgray", title)
graph_helper(nosignif_10$fstats, sigma_10, "f", color = "darkgray", title)
```

**Graph 1.2 - F Statistics for the `non-significant model` on each $\sigma$**

### Set of graphs for p-value distributions

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
title <- "p-values Distribution"
graph_helper(signif_1$pvals, sigma_1, "unif", color = "deepskyblue3", title)
graph_helper(signif_5$pvals, sigma_5, "unif", color = "deepskyblue3", title)
graph_helper(signif_10$pvals, sigma_10, "unif", color = "deepskyblue3", title)

```

**Graph 1.3 - p-values for the `significant model` on each $\sigma$**

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
title <- "p-values Distribution"
graph_helper(nosignif_1$pvals, sigma_1, "unif", color = "darkgray", title)
graph_helper(nosignif_5$pvals, sigma_5, "unif", color = "darkgray", title)
graph_helper(nosignif_10$pvals, sigma_10, "unif", color = "darkgray", title)
```

**Graph 1.4 - p-values for the `non-significant model` on each $\sigma$**

### Set of graphs for $R^2$ distributions

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
title <- "R^2 Distribution"
graph_helper(signif_1$r2s, sigma_1, color = "deepskyblue3", title = title)
graph_helper(signif_5$r2s, sigma_5, color = "deepskyblue3", title = title)
graph_helper(signif_10$r2s, sigma_10, color = "deepskyblue3", title = title)
```

**Graph 1.5 - $R^2$ for the `significant model` on each $\sigma$**

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
title <- "R^2 Distribution"
graph_helper(nosignif_1$r2s, sigma_1, color = "darkgray", title = title)
graph_helper(nosignif_5$r2s, sigma_5, color = "darkgray", title = title)
graph_helper(nosignif_10$r2s, sigma_10, color = "darkgray", title = title)
```

**Graph 1.6 - $R^2$ for the `non-significant model` on each $\sigma$**

## Discussion

**We expect empirical F-values to follow an F distribution under $H_0$.**

In this case the F-distribution has `r df1` and `r df2` degrees of freedom, we can see in the curve in orange for graphs `1.1` (`significant mode`l) and `1.2` (`non-significant model`).

Also, in the case of the `significant model`, when the noise is low ($\sigma=1$), the F-values from the simulation are relatively high moving away from the real curve. This translates to lower `p-values` and more successful `Rejections of the Null Hypothesis` which is what we expected since we know $\beta_i \neq 0$.

As $\sigma$ increases, the F-values become smaller and the empirical distribution resembles the real curve more and more. This makes more likely to `Fail to Reject the Null Hypothesis`.

In the case of the `non-significant model`, we expect to `Fail to Reject the Null Hypothesis` as much as we can, since we know $\beta_i = 0$. Regardless of the level of noise, the empirical distribution is close to the real curve (`Graph 1.2`).

**We expect the p-values to follow a uniform distribution under $H_0$.**

From `Graph 1.4` (`non-significant model`) we see the empirical distribution follows a `uniform distribution`, this is expected as in this case we know we should `Fail To Reject the Null Hypothesis`. So for any reasonable $\alpha$, it is likely that $\text{p-value} > \alpha$

As we observed for the F-values, we have a similar relation between the p-values distribution and the amount of noise $\sigma$. For the `significant model` when $\sigma=1$ we expect many small p-values so that is more likely to `Reject the null Hypothesis`. That is in fact the case in the first plot in `Graph 1.3`, there is a localized high density of very small p-values.

The following table summarizes the previous idea (given $\alpha=0.05$):

- For the significant model the ratio of *Rejections of $H_0$* depends on $\sigma$.
- While for the non-significant model we observed a very small ratio of *Rejections of $H_0$*.

```{r}

rejections <- data.frame(
  row.names = c("sigma=1", "sigma=5", "sigma=10"),
  "significant model" = c(mean(signif_1$pvals < 0.05),
                                       mean(signif_5$pvals < 0.05),
                                       mean(signif_10$pvals < 0.05)),
  "non-significant model" = c(mean(nosignif_1$pvals < 0.05),
                                              mean(nosignif_5$pvals < 0.05),
                                              mean(nosignif_10$pvals < 0.05))
)

gen_kable(rejections, 
          col_names = c("significant model", "non-significant model"),
          caption = "Proportion of Rejections of the Null Hypothesis, alpha=0.05")

```

**For the significant model, $R^2$ tends to be lower as $\sigma$ increases.**

Recall that the `Coefficient of Determination` $R^2$ is defined as:

\begin{aligned}
R^2 &= \frac{\text{SSReg}}{\text{SST}} = \frac{\sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} 
\end{aligned}

Our estimates $\hat{y}_i$ are likely farther from the real mean of the response $\bar{y}$ as we introduce more noise. That is, `SSReg` is smaller resulting in an also smaller $R^2$ value.

`Graph 1.5` (`significant model`) shows how $R^2$ distribution goes from left skewed to right skewed. We say then that the amount of variation explained by the regression model is reduced as we introduce more noise $\epsilon$.

In the case of the `non-significant model` we expect low $R^2$ values regardless of the level of noise. In other words, the non-significant model is not explaining the variation of the response as expected since we know $\beta_i = 0$. `Graph 1.6` shows only right skewed distributions due to low values of $R^2$ for all $\sigma$ values.

# Simulation Study 2: Using RMSE for Selection?

## Introduction

In this simulation study we will investigate how well RMSE can be used to select the “best” model. Since splitting the data is random, we don’t expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

We will use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These will be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time we simulate the data, we randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, we fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, we calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

We will repeat this process with $1000$ simulations for each of the $3$ values of $\sigma$. 

## Methods

Read the `csv` file with our samples and take a look to some of them.

```{r message=FALSE, warning=FALSE}
library(readr)
study2_data <- read_csv("./study_2.csv")

head(study2_data)

```

```{r}
# Set a seed to have a stable generation of random numbers
birthday = 19820426
set.seed(birthday)
```

We start by defining a few functions to help with the overall simulation. Namely, to help us with the tasks:

- Calculate the `RMSE`.
- Generate simulation data.
- Get the `RMSE` values and minimum values for each of the `9` models.

```{r}
rmse <- function(expected, predicted) {
  n <- length(expected)
  sqrt(sum((expected - predicted)^2) / n)
}

# Generate the simulation data, concretely the y values given an epsilon
# and the known model with 6 predictors.
simulate_data <- function(X, betas, sigma) {
  n <- nrow(X)
  epsilon <- rnorm(n = n, mean = 0, sd = sigma)
  X_full <- as.matrix(cbind(1, X[, 2:7]))
  X$y <- (X_full %*% betas) + epsilon
  as.data.frame(X)
}

# For each of the 9 models, this function calculates the RMSE 
# for train an test data as well as the index of the minimum RMSE on each case.
collect_rmse <- function(trn_data, tst_data) {
  # row 1 contains rmse values for train data on the 9 models
  # row 2 contains rmse values for test data on the 9 models
  rmse_matrix <- matrix(-1, 2, 9)

  for(im in 1:9) {
    # We take a bigger slice of the predictors until we reach 9
    trn_mod <- trn_data[, 1:(im+1)]
    tst_mod <- tst_data[, 1:(im+1)]
    # We train the model with the train chunk ONLY
    sim_model <- lm(y ~ ., data = trn_mod)
    rmse_matrix[1, im] <- rmse(trn_mod$y, sim_model$fitted.values)
    # We predict for the test chunk
    tst_predictions <- predict(sim_model, newdata = tst_mod)
    rmse_matrix[2, im] <- rmse(tst_mod$y, tst_predictions)
  }
  list(
    rmse_matrix = rmse_matrix,
    # The index indicates the model number with the smallest RMSE
    trn_min_idx = which.min(rmse_matrix[1, ]),
    tst_min_idx = which.min(rmse_matrix[2, ])
    )
}

```

Next, we pre-allocate variables we will use during the simulation.

```{r}
betas <- matrix(c(0, 3, -4, 1.6, -1.1, 0.7, 0.5))
sigma1 <- 1
sigma2 <- 2
sigma4 <- 4
num_sims <- 1000

rmse_s1_acc <- matrix(0, 2, 9)
rmse_s2_acc <- matrix(0, 2, 9)
rmse_s4_acc <- matrix(0, 2, 9)

# For each simulation we save the index of the model with the smallest RMSE
# We do that for both train and test data and each sigma
which_mins_s1 <- matrix(0, 2, num_sims)
which_mins_s2 <- matrix(0, 2, num_sims)
which_mins_s4 <- matrix(0, 2, num_sims)

```

The loop below runs the simulation and collects all the data. As described in the introduction: 

Each time we simulate the data, we randomly split the data into `train` and `test` sets of equal sizes (250 observations for training, 250 observations for testing).

Then for each model, we calculate Train and Test RMSE.

```{r}
# The main loop with num_sims simulations

for(i in 1:num_sims) {

  # We simulate the data for the true model (with 7 betas).
  # For each sigma we get a different data set.
  data_sigma1 <- simulate_data(study2_data, betas, sigma1)
  data_sigma2 <- simulate_data(study2_data, betas, sigma2)
  data_sigma4 <- simulate_data(study2_data, betas, sigma4)
  
  # Get 250 random indexes for observations
  idxs <- 1:nrow(study2_data)
  trn_idx = sample(idxs, 250)
  
  # Training splits for each sigma
  data_sigma1_trn <- data_sigma1[trn_idx, ]
  data_sigma2_trn <- data_sigma2[trn_idx, ]
  data_sigma4_trn <- data_sigma4[trn_idx, ]
  
  # Testing splits for each sigma
  data_sigma1_tst <- data_sigma1[!(idxs %in% trn_idx), ]
  data_sigma2_tst <- data_sigma2[!(idxs %in% trn_idx), ]
  data_sigma4_tst <- data_sigma4[!(idxs %in% trn_idx), ]
  
  # We get a list with test and train RMSE and the index with 
  # then min RMSE for both cases
  rmse_sigma1 <- collect_rmse(data_sigma1_trn, data_sigma1_tst)
  rmse_sigma2 <- collect_rmse(data_sigma2_trn, data_sigma2_tst)
  rmse_sigma4 <- collect_rmse(data_sigma4_trn, data_sigma4_tst)
  
  # Accumulate the rmse in a 2 by 9 matrix, 
  # we'll use it later to get the portion of correct model selection
  rmse_s1_acc <- rmse_s1_acc + rmse_sigma1$rmse_matrix
  rmse_s2_acc <- rmse_s2_acc + rmse_sigma2$rmse_matrix
  rmse_s4_acc <- rmse_s4_acc + rmse_sigma4$rmse_matrix
  
  # Save the index of the minimum RMSE for test/train and each sigma
  which_mins_s1[1, i] <- rmse_sigma1$trn_min_idx
  which_mins_s1[2, i] <- rmse_sigma1$tst_min_idx
  
  which_mins_s2[1, i] <- rmse_sigma2$trn_min_idx
  which_mins_s2[2, i] <- rmse_sigma2$tst_min_idx
  
  which_mins_s4[1, i] <- rmse_sigma4$trn_min_idx
  which_mins_s4[2, i] <- rmse_sigma4$tst_min_idx  

}


```


## Results

In this section we use a few graphs to summarize the results obtained in the simulations. The functions below will help us to:

- Plot the `Average RMSE` for `test` and `train` splits for each proposed `sigma`.
- Create a bar plot to show how many times each model got the lowest `RMSE`.

```{r}
line_plot <- function(y1, y2, sigma, title, show_ylab = FALSE) {
  ylab <- ifelse(show_ylab, paste("Average RMSE (", num_sims, "simulations )"), "")
  plot(1:9, y1,
       xlab = "Model size (number of predictors)", 
       ylab = ylab,
       main = bquote(.(title) ~ " (" ~ sigma  ~ "=" ~ .(sigma) ~")"), 
       col = "darkorange",
       lwd=2.5,
       lty=2,
       type='o')
  
  lines(1:9,y2,
        lwd=2.5,
        lty=2,        
        col='forestgreen',
        type='o')
  
  legend("topright", c("Train Data", "Test Data"), 
       lty = c(2, 2),
       lwd = 3,
       col = c("darkorange", "forestgreen"))  
}

bar_plot <- function(data, sigma) {
  td <- table(data)
  barplot(td,
    main=bquote(~"Model Selection based on RMSE (" ~ sigma ~ "=" ~ .(sigma) ~ ")"),
    xlab="Number of Predictors",
    ylab="Model Selection Count",
    border="gray",
    col='deepskyblue3',
    density=500,
    ylim = c(0, 600)
  )
}

```



We are interested in observing how the `RMSE` changes depending on the `number of predictors` or `model size`. We plot that for each of our `sigma` values.

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
t <- "Avg. RMSE vs Model Size"
line_plot(rmse_s1_acc[1, ] / num_sims, rmse_s1_acc[2, ] / num_sims,
          sigma = 1, title = t, show_ylab = TRUE)
line_plot(rmse_s2_acc[1, ] / num_sims, rmse_s2_acc[2, ] / num_sims,
          sigma = 2, title = t)
line_plot(rmse_s4_acc[1, ] / num_sims, rmse_s4_acc[2, ] / num_sims,
          sigma = 4, title = t)
```

**Graph 2.1 - Comparing Average RMSE for 3 sigma values**

Another important aspect to check is, which models are getting the `lowest` RMSE. The bar plots below show that for each `sigma` and the `test data`.

```{r fig.height = 6, fig.width = 10, fig.align = "center"}

par(mfrow=c(1, 3), bg="ghostwhite")
bar_plot(which_mins_s1[2, ], sigma1)
bar_plot(which_mins_s2[2, ], sigma2)
bar_plot(which_mins_s4[2, ], sigma4)
```

**Graph 2.2 - Fitted models with lowest RMSE**

The table below presents the exact proportion of `simulations` in which `model 6` (our true known model) is getting the lowest `RMSE`.

```{r}
rmse_sel <- c(mean(which_mins_s1[2, ] == 6),
              mean(which_mins_s2[2, ] == 6),
              mean(which_mins_s4[2, ] == 6)
              )

gen_kable(rmse_sel, col_names = c("Proportion"), 
          caption = "Proportion of Simulations selecting model 6.",
          row_names = c("sigma = 1", "sigma = 2", "sigma = 4"))

```

**Table 2.1 - Correct model selection for each** $\sigma$

Lastly, we notice below that in the case of the `train data`, the smallest `RMSE` always corresponds to the `largest model`, in this case the `model` with 9 `predictors`.

```{r}
# The values below correspond to the total number of simulations.
c(sum(which_mins_s1[1, ] == 9),
  sum(which_mins_s2[1, ] == 9),
  sum(which_mins_s4[1, ] == 9))

```


## Discussion

**The method of lowest RMSE does not always select the true model**

From `Table 2.1` we can see that the proportion of times we selected `model 6` is not `1`, meaning that in the experiment we are not always selecting the `true model`. For instance, for $\sigma = 1$ we select it only for `r (mean(which_mins_s1[2, ] == 6) * 100)`% of the `r num_sims` simulations.

Furthermore, from `Graph 2.2` we can see other models getting the lowest `RMSE` for different values of $\sigma$. Notably, the models close to the `true model` get sometimes the smallest `RMSE`.

`Graph 2.1` shows an important detail, the `Avg RMSE` seems to reach a minimum for `model 6` (the `true model`), after that, it tends to go slightly up. So we can state that in average the lowest `RMSE` will correspond to the `true model`.

**The level of noise affects the RMSE values**

Recall that the `RMSE` depends on the `Sum of Squared Error` which represents the unexplained variation of $y$. By introducing more noise (i.e. a larger $\sigma$) we are less confident of our estimates for $\hat{y}_i$, this increases the distance between our estimates and the true value of $y$. So the term $y_i - \hat{y}_i$ becomes bigger causing a bigger `RMSE` value.

$$
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
$$

From `Graph 2.1` we can see that the `Average RMSE` increases as we increase the $\sigma$ value. This is true for both `Train` and `Test` data.

Furthermore, more noise can also affect the selection of the `true model`. By looking at `Graph 2.2` we can see that as $\sigma$ increases, other models get selected more frequently.

Lastly, from `Graph 2.1` we can see that the Average `RMSE` for the `train data` only goes lower the more predictors are in the model, this a similar but opposite effect as the one observed for $R^2$, where it increases as more predictors are added to the model.

# Simulation Study 3: Power

## Introduction

In this simulation study we will investigate the **power** of the significance of regression test for simple linear regression. 

\[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

Recall, we had defined the *significance* level, $\alpha$, to be the probability of a Type I error.

\[
\alpha = P[\text{Reject } H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]
\]

Similarly, the probability of a Type II error is often denoted using $\beta$; however, this should not be confused with a regression parameter.

\[
\beta = P[\text{Fail to Reject } H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]
\]

*Power* is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

\[
\text{Power} = 1 - \beta = P[\text{Reject } H_0 \mid H_1 \text{ True}]
\]

Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

For each possible $\beta_1$ and $\sigma$ combination, we will simulate from the true model at least $1000$ times. Each time, we will perform the significance of the regression test. To estimate the power with these simulations, and some $\alpha$, use

\[
\hat{\text{Power}} = \hat{P}[\text{Reject } H_0 \mid H_1 \text{ True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}
\]

## Methods

Let's start pre-allocating the variables we will need during the simulation.

```{r}
birthday <- 19820426
set.seed(birthday)

alpha <- 0.05
num_sims <- 1000 
sigmas <- c(1, 2, 4)
# The 3 different sizes for the observations.
n_obs <- c(10, 20, 30)
# A sequence to get the beta_1 values.
betas_1 <- seq(-2, 2, 0.1)

# Contains our 3 sets of x values (with 1 predictor only)
x_values <- list(seq(0, 5, length = n_obs[1])
             ,seq(0, 5, length = n_obs[2])
             ,seq(0, 5, length = n_obs[3])
             )

# a matrix where each (i,j) is a boolean with the result of significance test for
# the simulation_i and the x_values_j data set.
significance_results <- matrix(0, num_sims, length(x_values))

# a matrix where each (i,j) is the power of a beta_i given a x_values_j.
betas_by_powers <- matrix(0, length(betas_1), length(x_values))

```

The following function returns the `p-value` for the `Significance of Regression`. In other words the `p-value` to test Hypothesis:

$$H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0$$

```{r}

# Fit a SLR model
sim_slr <- function(x, beta_0 = 0, beta_1, sigma) {
  n <- length(x)
  epsilon <- rnorm(n = n, mean = 0, sd = sigma)
  y <- beta_0 + beta_1 * x + epsilon
  lm(y ~ x)
}

# Extract the `p-value` we will use for testing the 
# Siginificance of the Regression test.
get_p_val <- function(model) {
  summary(model)$coef["x", "Pr(>|t|)"]
}

```

Now we can execute the experiment for each of our 3 $\sigma$ values. Notably, we need to keep count of the `Rejections` to calculate the power after every `r num_sims` simulations.

```{r}

run_simulation <- function(sigma) {
  # for each beta_1
  for (bidx in 1:length(betas_1)) {
    bi <- betas_1[bidx]
    for (si in 1:num_sims) {             
      
      # Fit a model for each set of observations.
      model_n10 <- sim_slr(x_values[[1]], beta_1=bi, sigma=sigma)
      model_n20 <- sim_slr(x_values[[2]], beta_1=bi, sigma=sigma)
      model_n30 <- sim_slr(x_values[[3]], beta_1=bi, sigma=sigma)
      p_values <- c(get_p_val(model_n10)
                    ,get_p_val(model_n20)
                    ,get_p_val(model_n30)
                    )
      
      # Store the test results for the ith simulation
      significance_results[si, ] <- (p_values < alpha)
    }
    # When simulations are finished for a beta, calculate the power
    powers <- colSums(significance_results) / num_sims
    # Store the powers for the ith beta
    betas_by_powers[bidx, ] <- powers
  }
  betas_by_powers
}

results_sigma1 <- run_simulation(sigmas[1])
results_sigma2 <- run_simulation(sigmas[2])
results_sigma4 <- run_simulation(sigmas[3])

```


## Results

From our simulation, we are interest in observing how the `Power` changes for the following factors:

- Values of $\beta_1$
- Values of $\sigma$ 
- The number of observations $n$


```{r}
power_plot <- function(x, y1, y2, y3, sigma) {
  plot(x, y1,
       xlab = beta[1] ~ " values", 
       ylab = paste("Signal Power (", num_sims, "simulations )"),
       main = bquote(~ "Power Vs " ~ beta[1] ~ " (" ~ sigma  ~ "=" ~ .(sigma) ~")"),
       col = "darkorange",
       lwd=2.5,
       lty=2,
       ylim= c(0,1),
       type='o')
  
  lines(x, y2,
        lwd=2.5,
        lty=2,        
        col='forestgreen',
        type='o')
  
  lines(x, y3,
        lwd=2.5,
        lty=2,        
        col='darkorchid4',
        type='o')  
  
  legend("bottomright", 
         c(paste(n_obs[1], "Observations"),
           paste(n_obs[2], "Observations"),
           paste(n_obs[3], "Observations")), 
       lty = c(3, 3, 3),
       lwd = 3,
       col = c("darkorange", "forestgreen", "darkorchid4"))  
}
```

```{r fig.height = 6, fig.width = 9, fig.align = "center"}
par(mfrow=c(1, 1), bg="ghostwhite")
power_plot(betas_1, 
           results_sigma1[ , 1],
           results_sigma1[ , 2],
           results_sigma1[ , 3],
           sigmas[1])
```

**Graph 3.1 - Power as a function of** $\beta_1$ **for 3 data sets**, $\sigma=1$

```{r fig.height = 6, fig.width = 9, fig.align = "center"}
par(mfrow=c(1, 1), bg="ghostwhite")
power_plot(betas_1, 
           results_sigma2[ , 1],
           results_sigma2[ , 2],
           results_sigma2[ , 3],
           sigmas[2])
```

**Graph 3.2 - Power as a function of** $\beta_1$ **for 3 data sets**, $\sigma=2$

```{r fig.height = 6, fig.width = 9, fig.align = "center"}
par(mfrow=c(1, 1), bg="ghostwhite")
power_plot(betas_1, 
           results_sigma4[ , 1],
           results_sigma4[ , 2],
           results_sigma4[ , 3],
           sigmas[3])
```

**Graph 3.3 - Power as a function of** $\beta_1$ **for 3 data sets**, $\sigma=4$

## Discussion

In the 3 graphs above we can notice a few things in common:

- **As $\beta_1$ approaches 0 the `Power` also approaches 0.**

We interpret this as increasing the probability of making `Type II` errors as $\beta_1$ gets smaller in magnitude. In other words, it is harder to detect the significance of a predictor if the parameter is small.

This is expected if we consider that with small values of $\beta_1$ and a fixed value of $sigma$ and degrees of freedom, it's likely to get small values for the `F statistic`, recall that for `SLR` we define it as:

$$F = \frac{\sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2 / 1}{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2 / (n - 2)}$$
In a simpler way, the smaller $\beta_1$ is the more likely we will `Fail to Reject the Null Hypothesis`, thus, the related predictor $x_1$ seems to have less and less significance.

- **With a larger number of observations the `Power` is more stable even if $\beta_1$ gets smaller or we introduce more noise to the simulation.**

This is an interesting point where is clear that having more observations help us to make less errors. If we for instance consider `Graph 3.1`, the `Power Curve for 30 observations` (in purple) stays more stable as $\beta_1$ gets smaller. In other words, having more observations increases the probability of detecting a signal.

We can see the same pattern repeating for `Graphs 3.2 and 3.3`.

Back to the formula to calculate `F`, when $n-2$ is larger (n being the number of observations) the `F` value would be larger.

- **The `Power Curves` drop faster when $\sigma$ is larger.**

A larger $\sigma$ introduces more uncertainty to the model so we are less confident of the model estimates. We are then making more `Type II` errors and having less Rejections of the `Null Hypothesis`.

`Graph 3.1` and `Graph 3.3` are an interesting contrast where is clear how the noise in the system can make easier or harder to detect a signal.

It is also interesting to think on these `Power Curves` as an upside down version of the `Normal Curve` for $\epsilon$.
