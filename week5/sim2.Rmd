---
title: 'Week 6 - Midterm Assignment: A Simulation Project'
author: "STAT 420, Summer 2021,Alejandro Pimentel (ap41)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
    code_folding: show
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

```{css, echo=FALSE}
p, li, td {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
```

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(knitr)
library(kableExtra)
library(dplyr)

format_numerics <- function(data, digits = 2, notation_threshold = 0.00001) {
  # make sure is a data.frame, then format
  if(!is.data.frame(data)){
    data <- as.data.frame(data)
  }  
  data %>% 
    mutate_if(
      is.numeric, 
      function(x) {
        if_else(
          abs(x) < notation_threshold, 
          formatC(x, digits = digits, format = "e"), 
          formatC(x, digits = digits, format = "f", drop0trailing = FALSE)
        )
      }
    )
}

gen_kable <- function(table_data, add_row_names = TRUE, caption = "", col_names = c(), row_names = c()) {
  f_data <- format_numerics(table_data) 
  if(length(col_names) != 0){
    colnames(f_data) <- col_names
  }
  if(length(row_names) != 0){
    rownames(f_data) <- row_names
  }  
  f_data %>%
  kable(., format = "html", row.names = add_row_names,
        caption = caption, escape = FALSE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"),
                  full_width = F,
                  font_size = 14,
                  position = "center")
}

fr <- function(nums, digits = 5, notation_threshold = 0.000001) {
  if_else(
    abs(nums) < notation_threshold, 
    formatC(nums, digits = digits, format = "e"), 
    format(round(nums, digits), nsmall = digits)
  )
}

```

# Simulation Study 2: Using RMSE for Selection?

## Introduction

In this simulation study we will investigate how well RMSE can be used to select the “best” model. Since splitting the data is random, we don’t expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

We will use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These will be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time we simulate the data, we randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, we fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, we calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

We will repeat this process with $1000$ simulations for each of the $3$ values of $\sigma$. 

## Methods

Read the `csv` file with our samples and take a look to some of them.

```{r message=FALSE, warning=FALSE}
library(readr)
study2_data <- read_csv("./study_2.csv")

head(study2_data)

```

```{r}
# Set a seed to have a stable generation of random numbers
birthday = 19820426
set.seed(birthday)
```

We start by defining a few functions to help with the overall simulation. Namely with the tasks:

- Calculate the RMSE.
- Generate simulation data.
- Get the RMSE values and minimum values for each of the `9` models.

```{r}
rmse <- function(expected, predicted) {
  n <- length(expected)
  sqrt(sum((expected - predicted)^2) / n)
}

# Generate the simulation data, concretely the y values given an epsilon
# and the known model with 6 predictors.
simulate_data <- function(X, betas, sigma) {
  n <- nrow(X)
  #print(X[, 2:7])
  epsilon <- rnorm(n = n, mean = 0, sd = sigma)
  X_full <- as.matrix(cbind(1, X[, 2:7]))
  X$y <- (X_full %*% betas) + epsilon
  as.data.frame(X)
}

# This function calculates the RMSE for train an test data as well
# as the index of the minimum RMSE on each case.
collect_rmse <- function(trn_data, tst_data) {
  rmse_matrix <- matrix(-1, 2, 9)

  for(im in 1:9) {
    # We take a bigger slice of the predictors until we reach 9
    trn_mod <- trn_data[, 1:(im+1)]
    tst_mod <- tst_data[, 1:(im+1)]
    # We train the model with the train chunk ONLY
    sim_model <- lm(y ~ ., data = trn_mod)
    rmse_matrix[1, im] <- rmse(trn_mod$y, sim_model$fitted.values)
    # We predict for the test chunk
    tst_predictions <- predict(sim_model, newdata = tst_mod)
    rmse_matrix[2, im] <- rmse(tst_mod$y, tst_predictions)
  }
  list(
    rmse_matrix = rmse_matrix,
    # The index tell use the model number with the smallest RMSE
    trn_min_idx = which.min(rmse_matrix[1, ]),
    tst_min_idx = which.min(rmse_matrix[2, ])
    )
}

```

Next, we pre-allocate variables we will use during the simulation.

```{r}
betas <- matrix(c(0, 3, -4, 1.6, -1.1, 0.7, 0.5))
sigma1 <- 1
sigma2 <- 2
sigma4 <- 4
num_sims <- 1000

rmse_s1_acc <- matrix(0, 2, 9)
rmse_s2_acc <- matrix(0, 2, 9)
rmse_s4_acc <- matrix(0, 2, 9)

which_mins_s1 <- matrix(0, 2, num_sims)
which_mins_s2 <- matrix(0, 2, num_sims)
which_mins_s4 <- matrix(0, 2, num_sims)

```

The loop below runs the simulation and collects all the data. As described in the introduction: 

Each time we simulate the data, we randomly split the data into `train` and `test` sets of equal sizes (250 observations for training, 250 observations for testing).

Then for each model, we calculate Train and Test RMSE.

```{r}
# The main loop with num_sims simulations

for(i in 1:num_sims) {

  # We simulate the data for the known model (with 7 betas).
  # For each sigma we get a different data set.
  data_sigma1 <- simulate_data(study2_data, betas, sigma1)
  data_sigma2 <- simulate_data(study2_data, betas, sigma2)
  data_sigma4 <- simulate_data(study2_data, betas, sigma4)
  
  # Get 250 random indexes for observations
  idxs <- 1:nrow(study2_data)
  trn_idx = sample(idxs, 250)
  #print(trn_idx)
  
  # Training splits for each sigma
  data_sigma1_trn <- data_sigma1[trn_idx, ]
  data_sigma2_trn <- data_sigma2[trn_idx, ]
  data_sigma4_trn <- data_sigma4[trn_idx, ]
  
  # Testing splits for each sigma
  data_sigma1_tst <- data_sigma1[!(idxs %in% trn_idx), ]
  data_sigma2_tst <- data_sigma2[!(idxs %in% trn_idx), ]
  data_sigma4_tst <- data_sigma4[!(idxs %in% trn_idx), ]
  
  # We get a list with test and train RMSE and the index with 
  # then min RMSE for both cases
  rmse_sigma1 <- collect_rmse(data_sigma1_trn, data_sigma1_tst)
  rmse_sigma2 <- collect_rmse(data_sigma2_trn, data_sigma2_tst)
  rmse_sigma4 <- collect_rmse(data_sigma4_trn, data_sigma4_tst)
  
  # Accumulate the rmse in a 2 by 9 matrix, 
  # we'll use it later to get the portion of correct model selection
  rmse_s1_acc <- rmse_s1_acc + rmse_sigma1$rmse_matrix
  rmse_s2_acc <- rmse_s2_acc + rmse_sigma2$rmse_matrix
  rmse_s4_acc <- rmse_s4_acc + rmse_sigma4$rmse_matrix
  
  # Allocate the index of the minimum RMSE for test/train and each sigma
  which_mins_s1[1, i] <- rmse_sigma1$trn_min_idx
  which_mins_s1[2, i] <- rmse_sigma1$tst_min_idx
  
  which_mins_s2[1, i] <- rmse_sigma2$trn_min_idx
  which_mins_s2[2, i] <- rmse_sigma2$tst_min_idx
  
  which_mins_s4[1, i] <- rmse_sigma4$trn_min_idx
  which_mins_s4[2, i] <- rmse_sigma4$tst_min_idx  

}


```


## Results

In this section we use a few graphs to summarize the results obtained in the simulations. The functions below will help us to:

- Plot the `Average RMSE` for `test` and `train` splits for each proposed `sigma`.
- Create a bar plot to show how many times a model got the lowest `RMSE`.

```{r}
line_plot <- function(y1, y2, sigma, title, show_ylab = FALSE) {
  ylab <- ifelse(show_ylab, paste("Average RMSE (", num_sims, "simulations )"), "")
  plot(1:9, y1,
       xlab = "Model size (number of predictors)", 
       ylab = ylab,
       main = paste(title," ( sigma =", sigma,")"),     
       col = "darkorange",
       lwd=2.5,
       lty=2,
       col.main='blue',
       type='o')
  
  lines(1:9,y2,
        lwd=2.5,
        lty=3,        
        col='cadetblue3',
        type='o')
  
  legend("topright", c("Train", "Test"), 
       lty = c(2, 3),
       lwd = 3,
       col = c("darkorange", "cadetblue3"))  
}

bar_plot <- function(data, sigma) {
  td <- table(data)
  barplot(td,
    main=paste("Model Selection based on RMSE (sigma = ", sigma," )"),
    xlab="Number of Predictors",
    ylab="Model Selection Count",
    border="gray",
    col='deepskyblue3',
    density=500,
    ylim = c(0, 600)
  )
}

```



We are interested in observing how the RMSE changes depending on the `number of predictors` or `model size`. We plot that for each of our `sigma` values.

```{r fig.height = 6, fig.width = 10, fig.align = "center"}
par(mfrow=c(1, 3), bg="ghostwhite")
t <- "Avg. RMSE vs Model Size"
line_plot(rmse_s1_acc[1, ] / num_sims, rmse_s1_acc[2, ] / num_sims,
          sigma = 1, title = t, show_ylab = TRUE)
line_plot(rmse_s2_acc[1, ] / num_sims, rmse_s2_acc[2, ] / num_sims,
          sigma = 2, title = t)
line_plot(rmse_s4_acc[1, ] / num_sims, rmse_s4_acc[2, ] / num_sims,
          sigma = 4, title = t)
```

**Graph 2.1 - Comparing Average RMSE for 3 sigma values**

Another important aspect to check is, which models are getting the `lowest` RMSE. The bar plots below show that for each `sigma` and the `test data`.

```{r fig.height = 6, fig.width = 10, fig.align = "center"}

par(mfrow=c(1, 3), bg="ghostwhite")
bar_plot(which_mins_s1[2, ], sigma1)
bar_plot(which_mins_s2[2, ], sigma2)
bar_plot(which_mins_s4[2, ], sigma4)
```

**Graph 2.2 - Fitted models with lowest RMSE**

The table below presents the exact proportion of `simulations` in which `model 6` (our true known model) is getting the lowest `RMSE`.

```{r}
rmse_sel <- c(mean(which_mins_s1[2, ] == 6),
              mean(which_mins_s2[2, ] == 6),
              mean(which_mins_s4[2, ] == 6)
              )

gen_kable(rmse_sel, col_names = c("Proportion"), 
          caption = "Proportion of Simulations selecting model 6.",
          row_names = c("sigma = 1", "sigma = 2", "sigma = 4"))

```

**Table 2.1 - Correct model selection for each** $\sigma$

Lastly, we notice below that in the case of the `train data`, the smallest RMSE always corresponds to the `largest model`, in this case the `model` with 9 `predictors`.

```{r}
# The values below correspond to the total number of simulations.
sum(which_mins_s1[1, ] == 9)
sum(which_mins_s2[1, ] == 9)
sum(which_mins_s4[1, ] == 9)

```


## Discussion

**The method of lowest RMSE does not select the true model always**

From `Table 2.1` we can see that the proportion of times we selected `model 6` is not `1`, meaning that in the experiment we are not always selecting the `true model`. For instance, for $\sigma = 1$ we select it only for `r (mean(which_mins_s1[2, ] == 6) * 100)`% of the `r num_sims` simulations.

Furthermore, from `Graph 2.2` we can see other models getting the lowest `RMSE` for different values of $\sigma$. Notably, the models around the 

